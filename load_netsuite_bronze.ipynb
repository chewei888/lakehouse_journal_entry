{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58829260-bd79-4d19-a95f-8b98bc63197f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Fetch Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a269df04-09da-4d94-a122-d7c15df6b277",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Fetch credentials from Azure Key Vault\n",
    "ACCOUNT_ID      = dbutils.secrets.get(scope=\"azure_key_vault\", key=\"NS-SB-ACCOUNT-ID\")\n",
    "VERSION         = \"v1\"\n",
    "REALM           = dbutils.secrets.get(scope=\"azure_key_vault\", key=\"NS-SB-REALM\")\n",
    "CONSUMER_KEY    = dbutils.secrets.get(scope=\"azure_key_vault\", key=\"NS-SB-CONSUMER-KEY\")\n",
    "CONSUMER_SECRET = dbutils.secrets.get(scope=\"azure_key_vault\", key=\"NS-SB-CONSUMER-SECRET\")\n",
    "TOKEN_KEY       = dbutils.secrets.get(scope=\"azure_key_vault\", key=\"NS-SB-TOKEN-ID\")\n",
    "TOKEN_SECRET    = dbutils.secrets.get(scope=\"azure_key_vault\", key=\"NS-SB-TOKEN-SECRET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d73a531-e90f-4cf2-a2d6-cc09d5e46a15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Define Logging and Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "022ad726-b852-4b77-a6ee-4d0690d5c7f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import logging\n",
    "\n",
    "# Configure the root logger’s level & format\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format= \"%(asctime)s %(levelname)s %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Grab the root logger once for the whole module\n",
    "root_logger = logging.getLogger()\n",
    "\n",
    "# Define a custom ListHandler. Handler that appends each record to the target list.\n",
    "class ListHandler(logging.Handler):\n",
    "    def __init__(self, target_list):\n",
    "        super().__init__() # under the hood invokes logging.Handler.__init__\n",
    "        self.target = target_list\n",
    "\n",
    "    def emit(self, record): # “record” is the LogRecord created by logging.info()/error()        \n",
    "        utc_dt = datetime.fromtimestamp(\n",
    "            record.created, # a UTC-based POSIX timestamp\n",
    "            tz=ZoneInfo(\"UTC\") # keep in UTC\n",
    "        )\n",
    "\n",
    "        # Append structured log entry\n",
    "        self.target.append({\n",
    "            \"run_ts\": utc_dt,\n",
    "            \"level\": record.levelname,\n",
    "            \"message\": record.getMessage()\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc808843-55ac-493b-b5e2-968313b6cd7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c065eecd-a2e1-482f-9e9d-16c77c467269",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests_oauthlib import OAuth1\n",
    "\n",
    "def set_netsuite_connection(\n",
    "    account_id: str,\n",
    "    version: str,\n",
    "    realm: str,\n",
    "    limit: int = 1000,\n",
    "    params: dict = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Build REST endpoint, OAuth1 auth, headers, and default params.\n",
    "    \"\"\"\n",
    "    # Build the base URL for REST calls\n",
    "    base_url = f\"https://{account_id}.suitetalk.api.netsuite.com/services/rest/record/{version}/\"\n",
    "\n",
    "    # Set authentication\n",
    "    oauth = OAuth1(\n",
    "        client_key=CONSUMER_KEY,\n",
    "        client_secret=CONSUMER_SECRET,\n",
    "        resource_owner_key=TOKEN_KEY,\n",
    "        resource_owner_secret=TOKEN_SECRET,\n",
    "        signature_method=\"HMAC-SHA256\",\n",
    "        realm=realm\n",
    "    )\n",
    "\n",
    "    # Standard NetSuite REST headers\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Set parameters. Clone params per call to avoid shared mutable state in threads\n",
    "    call_params = dict(params) if params else {}\n",
    "    call_params.setdefault(\"limit\", limit) # only assigns limit if the key \"limit\" is not already present in the call_params dict.\n",
    "    \n",
    "    return base_url, oauth, headers, call_params\n",
    "\n",
    "\n",
    "def get_all_netsuite_page(endpoint: str, base_url: str, oauth: OAuth1, headers: dict, params: dict) -> list:\n",
    "    \"\"\"\n",
    "    Paginate through NetSuite list endpoint, collecting all item summaries.\n",
    "    \"\"\"\n",
    "    # Set url\n",
    "    url = base_url + endpoint\n",
    "    \n",
    "    # Log the start    \n",
    "    logging.info(\"Starting GET all NetSuite Pages for endpoint '%s' with parameters %s at URL: %s\", endpoint, params, url)\n",
    "    \n",
    "    # Final result\n",
    "    all_page_items = []\n",
    "\n",
    "    while True:\n",
    "        # Get a page\n",
    "        try:\n",
    "            page_resp = requests.get(url=url, auth=oauth, headers=headers, params=params)\n",
    "            logging.info(\"Received HTTP %d for page URL: %s\", page_resp.status_code, url)\n",
    "            page_resp.raise_for_status()\n",
    "        except requests.RequestException as e:\n",
    "            logging.error(\"GET page request to %s failed: %s\", url, e)\n",
    "            raise\n",
    "\n",
    "        # Transform the page response into json\n",
    "        page = page_resp.json()\n",
    "\n",
    "        # Collect all item summaries (just id + links) to the result\n",
    "        all_page_items.extend(page.get(\"items\", []))\n",
    "\n",
    "        # Get next link\n",
    "        next_link = [link for link in page.get(\"links\", []) if link.get(\"rel\") == \"next\"]\n",
    "        \n",
    "        # Find any \"next\" link\n",
    "        if not next_link:\n",
    "            logging.info(\"No next link found. Completed GET all NetSuite Pages.\")\n",
    "            break\n",
    "        \n",
    "        # Move to the next page URL\n",
    "        next_url = next_link[0][\"href\"]\n",
    "        logging.info(\"Found next link, will request: %s\", next_url)\n",
    "        url = next_url\n",
    "\n",
    "        # Clear the parameters on subsequent calls because they inherit from initial parameters\n",
    "        params = {}\n",
    "\n",
    "    return all_page_items\n",
    "\n",
    "\n",
    "def get_netsuite_page_details(all_page_items: list, oauth: OAuth1, headers: dict) -> list:\n",
    "    \"\"\"\n",
    "    Fetch full JSON for each item summary.\n",
    "    \"\"\"\n",
    "    # Log the start\n",
    "    logging.info(\"Starting GET NetSuite page details\")\n",
    "\n",
    "    # Final result\n",
    "    all_item_details = []\n",
    "    \n",
    "    # If empty input, retrun empty result immediately\n",
    "    if not all_page_items:\n",
    "        logging.info(\"No pages found. End.\")\n",
    "        return all_item_details\n",
    "    \n",
    "    # Get item details\n",
    "    for item in all_page_items:\n",
    "        # Fetch item id\n",
    "        item_id = item.get(\"id\", \"(no-id)\")\n",
    "\n",
    "        # Fetch the item url\n",
    "        item_detail_url = item[\"links\"][0][\"href\"]\n",
    "\n",
    "        # Fetch the item details by item url (single page for each item)\n",
    "        try:\n",
    "            # Get response\n",
    "            item_resp = requests.get(url=item_detail_url, auth=oauth, headers=headers) # no limit parameter\n",
    "\n",
    "            # Log info message\n",
    "            logging.info(\"Received HTTP %d for item URL: %s\", item_resp.status_code, item_detail_url)\n",
    "            item_resp.raise_for_status()\n",
    "\n",
    "            # Transform the item response to json\n",
    "            item_detail = item_resp.json()\n",
    "\n",
    "            # Append the details to result\n",
    "            all_item_details.append(item_detail)\n",
    "        except requests.RequestException as e:\n",
    "            logging.error(\"GET item request to %s failed: %s\", item_detail_url, e)\n",
    "            continue # continue to the next item if current item failed\n",
    "\n",
    "    logging.info(\"Completed GET NetSuite page details\")\n",
    "    return all_item_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b93622b4-cd8b-44fb-8c8f-d451cf4ca619",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_to_lakehouse(\n",
    "    data: list            =None, \n",
    "    partition_number: int =1, \n",
    "    medal: str            =\"bronze\",\n",
    "    subfolder_path: str   =\"netsuite\", \n",
    "    entity: str           =\"\",\n",
    "    file_format: str      =\"delta\", \n",
    "    write_mode: str       =\"overwrite\", \n",
    "    merge_schema: bool    =\"True\"\n",
    "):  \n",
    "    \"\"\"\n",
    "    Write a list of JSON records into a Delta table in ADLS and register the table in Unity Catalog.\n",
    "    \"\"\"\n",
    "    # Check whether the data is empty\n",
    "    if not data:\n",
    "        logging.info(\"No data for %s. Skip write.\", entity)\n",
    "        return\n",
    "\n",
    "    # Build a RDD\n",
    "    json_rdd = sc.parallelize([json.dumps(record) for record in data])\n",
    "\n",
    "    # Read RDD as json\n",
    "    df = spark.read.json(json_rdd)\n",
    "\n",
    "    # Define the storage path\n",
    "    path = f\"abfss://{medal}@qydatalake.dfs.core.windows.net/{subfolder_path}/{entity}/\"\n",
    "\n",
    "    # Define the catalog name and table name\n",
    "    subfolder_path = subfolder_path.strip(\"/\") # Remove the leading or trailing \"/\"\n",
    "    parts = subfolder_path.split(\"/\") # Splits the cleaned path into a list of directory names.\n",
    "    catalog_name = parts[0] # take the first part as catalog name\n",
    "    if parts[1:]:\n",
    "        postfix = \"_\".join(parts[1:]) # take the rest of part (if exists) as postfix of table name\n",
    "        table_name = f\"{entity}_{postfix}\"\n",
    "    else:\n",
    "        table_name = entity\n",
    "\n",
    "    # Write into lakehouse\n",
    "    (\n",
    "        df\n",
    "        .coalesce(partition_number) # data is small (< 250 MB), collapse to one file to avoid metadata scan overhead\n",
    "        .write\n",
    "        .format(file_format)\n",
    "        .mode(write_mode)\n",
    "        .option(\"mergeSchema\", merge_schema) # allows new columns to be added when the table already exists\n",
    "        .option(\"path\", path) # writes the Delta table into ADLS\n",
    "        .saveAsTable(f\"{catalog_name}.{medal}.{table_name}\") # registers (or refreshes) the table in Unity Catalog on Databricks\n",
    "    )\n",
    "\n",
    "    logging.info(\"Wrote %d records of %s to %s\", len(data), entity, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e89aff64-17f7-494e-a60b-b73b7ec951eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType, StructField, StructType, StringType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def log_progress(\n",
    "    log_records: list     =None,\n",
    "    medal: str            =\"bronze\",\n",
    "    subfolder_path: str   =\"netsuite\", \n",
    "    entity: str           =\"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Persist captured logs into Delta under ADLS.\n",
    "    \"\"\"\n",
    "    if not log_records:\n",
    "        return\n",
    "\n",
    "    # Define the log path\n",
    "    log_path = f\"abfss://{medal}@qydatalake.dfs.core.windows.net/{subfolder_path}/logs/{entity}_log\"\n",
    "\n",
    "    # Define the log schema\n",
    "    schema = StructType([\n",
    "        StructField(\"run_ts\", TimestampType(), nullable=False),\n",
    "        StructField(\"level\", StringType(), nullable=False),\n",
    "        StructField(\"message\", StringType(), nullable=False)\n",
    "    ])\n",
    "\n",
    "    # Create the log dataframe with exact one file output\n",
    "    log_df = spark.createDataFrame(log_records, schema).coalesce(1)\n",
    "\n",
    "    # Write the log\n",
    "    (\n",
    "        log_df\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .option(\"mergeSchema\", True)\n",
    "        .save(log_path)\n",
    "    )\n",
    "\n",
    "    logging.info(\"Logged %d entries for %s\", len(log_records), entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c457b91-f8e0-49f2-b7e7-2f247e149de3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_entity_data(\n",
    "    entity: str, base_url: str, oauth: OAuth1, headers: dict, params: dict\n",
    "):\n",
    "    \"\"\"\n",
    "    Thread-safe wrapper: fetch items & details, capture logs, return data + logs.\n",
    "    \"\"\"\n",
    "    entity_logs = []\n",
    "    list_handler = ListHandler(entity_logs)\n",
    "    list_handler.setLevel(logging.INFO)\n",
    "    root_logger.addHandler(list_handler)\n",
    "\n",
    "    try:\n",
    "        # 1. Fetch all pages (just returns the “item” summaries: id + self‐link)\n",
    "        page_items = get_all_netsuite_page(endpoint=entity, base_url=base_url, oauth=oauth, headers=headers, params=params)\n",
    "        # 2. Fetch full JSON for each “item” in every page (a list of dicts)\n",
    "        page_item_details = get_netsuite_page_details(all_page_items=page_items, oauth=oauth, headers=headers)\n",
    "    finally:\n",
    "        root_logger.removeHandler(list_handler)\n",
    "\n",
    "    return entity, page_item_details, entity_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2174fb6f-369b-4aa9-aca3-9c779fbd2081",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# 1. Fetch entities from NetSuite by REST API\n",
    "# 2. Write the output to Lakehouse\n",
    "# 3. Log progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "887aaa95-3c1b-46a3-8d46-e4a7332a5b37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# === MAIN PIPELINE ===\n",
    "if __name__ == \"__main__\":\n",
    "    # Prepare NetSuite connection\n",
    "    base_url, oauth, headers, params = set_netsuite_connection(account_id=ACCOUNT_ID, version=VERSION, realm=REALM)\n",
    "\n",
    "    # Entities we want to load\n",
    "    entities = [\"account\", \"subsidiary\", \"department\"]\n",
    "\n",
    "    # Parallel HTTP fetch\n",
    "    with ThreadPoolExecutor(max_workers=len(entities)) as pool:\n",
    "        futures = {\n",
    "            pool.submit(fetch_entity_data, entity, base_url, oauth, headers, params): entity\n",
    "            for entity in entities\n",
    "        }\n",
    "\n",
    "        for future in as_completed(futures): # futures is a Dict[Future, str]\n",
    "            # entity = futures[future]\n",
    "            entity, data, logs = future.result()\n",
    "\n",
    "            # Write to Lakehouse\n",
    "            write_to_lakehouse(data=data, partition_number=1, medal=\"bronze\", subfolder_path=\"netsuite\", entity=entity)\n",
    "\n",
    "            # Log progress\n",
    "            log_progress(log_records=logs, medal=\"bronze\", subfolder_path=\"netsuite\", entity=entity)\n",
    "\n",
    "# --- End of netsuite_loader.py ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b10fdda9-2688-435d-82d9-7a11776ee6ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "load_netsuite_bronze",
   "widgets": {}
  },
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "889fd91e-de83-4fb6-b1e0-632befda4c9e",
    "default_lakehouse_name": "qylakehouse",
    "default_lakehouse_workspace_id": "0679a537-5393-4d69-9dfc-4c327dba5458",
    "known_lakehouses": [
     {
      "id": "889fd91e-de83-4fb6-b1e0-632befda4c9e"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}