{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7be0c7e0-7974-4ca1-aea7-16570966a721",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Configuration & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f7953e2-81e9-4343-94b4-3131e8c40e3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# === Configuration & Constants ===\n",
    "\n",
    "# Key Vault scope and NetSuite credential keys mapping\n",
    "VAULT_SCOPE = \"azure_key_vault\"\n",
    "NS_CREDENTIAL_KEYS = {\n",
    "    'account':              'NS-SB-ACCOUNT-ID',\n",
    "    'realm':                'NS-SB-REALM',\n",
    "    'consumer_key':         'NS-SB-CONSUMER-KEY',\n",
    "    'consumer_secret':      'NS-SB-CONSUMER-SECRET',\n",
    "    'token_key':            'NS-SB-TOKEN-ID',\n",
    "    'token_secret':         'NS-SB-TOKEN-SECRET'\n",
    "}\n",
    "VERSION = \"v1\"\n",
    "\n",
    "# Construct ADLS paths for source files and processed log\n",
    "MEDAL = \"bronze\"\n",
    "SUBFOLDER = \"paycor\"\n",
    "ENTITY = \"journalEntry\"\n",
    "BASE_PATH = f\"abfss://{MEDAL}@qydatalake.dfs.core.windows.net/{SUBFOLDER}\"\n",
    "FILE_DIR = f\"{BASE_PATH}/{ENTITY}/\"\n",
    "LOG_DIR = f\"{BASE_PATH}/logs/{ENTITY}_processed/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99742d24-5710-455d-8338-af732220053c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Set Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "109e303a-23c5-425d-adf1-42cb3847a65d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 01:47:45 INFO Received command c on object id p1\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import logging\n",
    "from logging.handlers import BufferingHandler\n",
    "\n",
    "# Configure the root loggerâ€™s level & format\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format= \"%(asctime)s %(levelname)s %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Grab the root logger once for the whole module\n",
    "root_logger = logging.getLogger()\n",
    "root_logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b55eaf7c-d831-4b6a-8e05-91a016c87f1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aea9269-8264-42df-831a-3994efb31c82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# === Utility Functions ===\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, DateType, BooleanType, TimestampType\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "def list_unprocessed_files(file_path: str, file_log_path: str, extension: str = \".dat\"):\n",
    "    \"\"\"\n",
    "    List files in ADLS directory, filter by extension, and exclude those\n",
    "    already marked as processed in the Delta log table.\n",
    "    \"\"\"\n",
    "    # 1. list files\n",
    "    files = [(f.name, ) for f in dbutils.fs.ls(file_path) if f.name.endswith(extension)]\n",
    "\n",
    "    # 2. build DataFrame\n",
    "    filename_df = spark.createDataFrame(files, [\"filename\"])\n",
    "\n",
    "    # 3. read or init log\n",
    "    try:\n",
    "        filelog_df = spark.read.format(\"delta\").load(file_log_path) # read processed log if exists\n",
    "    except AnalysisException:\n",
    "        log_schema = StructType([\n",
    "            StructField(\"filename\", StringType(), False),\n",
    "            StructField(\"filePath\", StringType(), False),\n",
    "            StructField(\"isProcessed\", BooleanType(), False),\n",
    "            StructField(\"processedAt\", TimestampType(), False)         \n",
    "        ])\n",
    "\n",
    "        filelog_df = spark.createDataFrame([], schema=log_schema) # create an empty processed log if not exists\n",
    "\n",
    "    # 4. subtract processed\n",
    "    unprocessed_df = (\n",
    "        filename_df\n",
    "        .join(filelog_df.filter(F.col(\"isProcessed\") == True), on=\"filename\", how=\"left_anti\")\n",
    "    )\n",
    "\n",
    "    return unprocessed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b1a5571-ccef-4726-833f-17545919445b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 01:47:45 INFO Received command c on object id p1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def compose_journal_entry_post_body(file_path: str, schema: StructType) -> dict:\n",
    "    \"\"\"\n",
    "    Read a CSV file for one journal entry, enrich with lookup tables,\n",
    "    and construct the JSON payload for NetSuite API.\n",
    "    \"\"\"\n",
    "    # Read raw CSV with schema\n",
    "    raw_df = (\n",
    "        spark.read\n",
    "        .format(\"csv\")\n",
    "        .option(\"header\", True)\n",
    "        .option(\"dateFormat\", \"dd-MMM-yy\") # e.g. 28-Mar-25\n",
    "        .schema(schema)\n",
    "        .load(file_path)\n",
    "    )\n",
    "\n",
    "    # Load and clean lookup tables\n",
    "    account_df = (\n",
    "        spark\n",
    "        .read\n",
    "        .table(\"netsuite.bronze.account\")\n",
    "    )\n",
    "\n",
    "    subsidiary_df = (\n",
    "        spark\n",
    "        .read\n",
    "        .table(\"netsuite.bronze.subsidiary\")\n",
    "        .withColumn(\"sub\", F.regexp_replace(F.col(\"name\"), r\"[^A-Za-z0-9]+\", \" \"))\n",
    "    )\n",
    "\n",
    "    department_df = (\n",
    "        spark\n",
    "        .read\n",
    "        .table(\"netsuite.bronze.department\")\n",
    "    )\n",
    "\n",
    "    # Normalize and join dimensions\n",
    "    df = (\n",
    "        raw_df\n",
    "        .withColumn(\"Account\", \n",
    "            F.when(\n",
    "                (F.instr(\"Account\", \".\") > 0) & (F.length(\"Account\") < 7),\n",
    "                F.rpad(\"Account\", 7, \"0\") # right pad Account to len 7 with tailing 0 if it has decimal point, such as 360.02 -> 360.020\n",
    "            ).otherwise(F.col(\"Account\"))\n",
    "        )\n",
    "        .withColumn(\"sub\", F.regexp_replace(\n",
    "                F.element_at(F.split(\"Subsidiary\", \" : \"), -1),\n",
    "                r\"[^A-Za-z0-9]+\", \n",
    "                \" \"\n",
    "            )\n",
    "        )\n",
    "        .withColumn(\"depName\", F.element_at(F.split(\"Cost_Centre\", \" : \"), -1))\n",
    "        .join(account_df.select(\"acctNumber\", F.col(\"id\").alias(\"account_id\")), on=F.col(\"Account\") == account_df[\"acctNumber\"], how=\"left\")\n",
    "        .join(subsidiary_df.select(\"sub\", F.col(\"id\").alias(\"subsidiary_id\")), on=\"sub\", how=\"left\")\n",
    "        .join(department_df.select(F.col(\"name\").alias(\"depName\"), F.col(\"id\").alias(\"department_id\")), on=\"depName\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    # Collect rows and validate\n",
    "    rows = df.collect()\n",
    "    if not rows:\n",
    "        logging.error(f\"No journal entries found in {file_path}\")\n",
    "        raise ValueError(f\"No journal entries found in {file_path}\")\n",
    "\n",
    "    # Pick the first row as the header (Assume one file only contains one journal entry)\n",
    "    hdr = rows[0]\n",
    "\n",
    "    # Set the top level attributes\n",
    "    tranDate = hdr[\"Date\"].strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    reversalDate = None\n",
    "    if hdr[\"Reversal_Date\"]:\n",
    "        reversalDate = hdr[\"Reversal_Date\"].strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    exchangeRate = 1.0\n",
    "\n",
    "    custbody_je_approver_id = \"4597\" # default as Eric\n",
    "\n",
    "    customForm_id = \"149\" # default as \"Quay Journal Entry\"\n",
    "\n",
    "    # Set the line item level attributes\n",
    "    items = []\n",
    "\n",
    "    for row in rows:\n",
    "        item ={\n",
    "                \"account\": {\n",
    "                    \"id\": row[\"account_id\"]\n",
    "                },\n",
    "                \"department\": {\n",
    "                    \"id\": row[\"department_id\"]\n",
    "                },\n",
    "                # \"cseg1\": {\n",
    "                #     \"refName\": row[\"Cost_Centre\"]\n",
    "                # },\n",
    "                \"memo\": row[\"Line_Memo\"]\n",
    "            }\n",
    "        \n",
    "        if row[\"Debit\"] and float(row[\"Debit\"]) != 0.0:\n",
    "            item[\"debit\"] = row[\"Debit\"]\n",
    "        else:\n",
    "            item[\"credit\"] = row[\"Credit\"]\n",
    "        \n",
    "        items.append(item)\n",
    "\n",
    "    # Compose the body\n",
    "    body = {\n",
    "        \"tranDate\": tranDate,\n",
    "        \"externalId\": hdr[\"External_ID\"],\n",
    "        \"memo\": hdr[\"Memo\"],\n",
    "        \"subsidiary\": {\n",
    "            \"id\": hdr[\"subsidiary_id\"]\n",
    "        },\n",
    "        \"currency\": {\n",
    "            \"refName\": hdr[\"Currency\"]\n",
    "        },\n",
    "        \"exchangeRate\": exchangeRate,\n",
    "        \"custbody_je_approver\": {\n",
    "            \"id\": custbody_je_approver_id\n",
    "        },\n",
    "        \"customForm\": {\n",
    "            \"id\": customForm_id\n",
    "        },\n",
    "        \"line\": {\n",
    "            \"items\": items\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if reversalDate:\n",
    "        body[\"reversalDate\"] = reversalDate\n",
    "\n",
    "    return body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29e7703e-c9f4-4abf-a859-2dfe7eed6c40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 01:47:45 INFO Received command c on object id p1\n2025-07-08 01:47:45 INFO Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from requests_oauthlib import OAuth1\n",
    "\n",
    "def fetch_secret(key: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve a secret value from Azure Key Vault using Databricks utilities.\n",
    "    \"\"\"\n",
    "    return dbutils.secrets.get(scope=VAULT_SCOPE, key=NS_CREDENTIAL_KEYS[key])\n",
    "\n",
    "def set_netsuite_connection(\n",
    "    version: str = VERSION,\n",
    "    params: dict = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Build REST endpoint, OAuth1 auth, headers, and default params.\n",
    "    \"\"\"\n",
    "    creds = {k: fetch_secret(k) for k in NS_CREDENTIAL_KEYS.keys()}\n",
    "    account = creds[\"account\"]\n",
    "\n",
    "    # Build the base URL for REST calls\n",
    "    base_url = f\"https://{account}.suitetalk.api.netsuite.com/services/rest/record/{version}/\"\n",
    "    \n",
    "    # Set authentication\n",
    "    oauth = OAuth1(\n",
    "        client_key=creds[\"consumer_key\"],\n",
    "        client_secret=creds[\"consumer_secret\"],\n",
    "        resource_owner_key=creds[\"token_key\"],\n",
    "        resource_owner_secret=creds[\"token_secret\"],\n",
    "        signature_method=\"HMAC-SHA256\",\n",
    "        realm=creds[\"realm\"]\n",
    "    )\n",
    "\n",
    "    # Standard NetSuite REST headers\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Set parameters. Clone params per call to avoid shared mutable state in threads\n",
    "    call_params = dict(params) if params else {}\n",
    "    \n",
    "    return base_url, oauth, headers, call_params\n",
    "\n",
    "def post_netsuite(endpoint: str, body: dict, base_url: str, oauth: OAuth1, headers: dict, params: dict) -> list:\n",
    "    \"\"\"\n",
    "    Send a POST request to NetSuite REST API and validate response.\n",
    "    \"\"\"\n",
    "    # Set url\n",
    "    url = base_url + endpoint\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, auth=oauth, headers=headers, params=params, json=body)\n",
    "        response.raise_for_status() # Will raise HTTPError for 4xx / 5xx\n",
    "    except requests.HTTPError as http_err: # You *will* get here on 4xx/5xx\n",
    "        status = http_err.response.status_code\n",
    "        error_message = response.json()\n",
    "        logging.error(f\"NetSuite returned {status} for POST {endpoint}: {error_message}\")\n",
    "        raise\n",
    "    except requests.RequestException as e: # Network errors, timeouts, etc.\n",
    "        logging.error(f\"Network error POSTing to NetSuite {endpoint}: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # only get here on 2xx\n",
    "    logging.info(f\"POST {endpoint} succeeded with {response.status_code}\")\n",
    "    return response.status_code\n",
    "\n",
    "def log_progress(\n",
    "    log_records: list     =None,\n",
    "    medal: str            =\"bronze\",\n",
    "    subfolder_path: str   =\"paycor\", \n",
    "    entity: str           =\"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Persist captured logs into Delta in ADLS\n",
    "    \"\"\"\n",
    "    if not log_records:\n",
    "        return\n",
    "\n",
    "    # Define the log path\n",
    "    log_path = f\"abfss://{medal}@qydatalake.dfs.core.windows.net/{subfolder_path}/logs/{entity}_log\"\n",
    "\n",
    "    # Define the log schema\n",
    "    schema = StructType([\n",
    "        StructField(\"run_ts\", TimestampType(), nullable=False),\n",
    "        StructField(\"level\", StringType(), nullable=False),\n",
    "        StructField(\"message\", StringType(), nullable=False)\n",
    "    ])\n",
    "\n",
    "    # Create the log dataframe with exact one file output\n",
    "    log_df = spark.createDataFrame(log_records, schema).coalesce(1)\n",
    "\n",
    "    # Write the log\n",
    "    (\n",
    "        log_df\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .option(\"mergeSchema\", True)\n",
    "        .save(log_path)\n",
    "    )\n",
    "\n",
    "    logging.info(\"Logged %d entries for %s\", len(log_records), entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eae8ad05-9dcb-4a19-acc4-2c1ec95142a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 01:47:45 INFO Received command c on object id p1\n"
     ]
    }
   ],
   "source": [
    "# === MAIN PIPELINE ===\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main pipelines: gathers unprocessed journal entry files from ADLS, processes them, and posts.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Identify new files to process\n",
    "        unprocessed_df = list_unprocessed_files(FILE_DIR, LOG_DIR)\n",
    "\n",
    "        # Log if no files to process\n",
    "        if unprocessed_df.rdd.isEmpty():\n",
    "            logging.info(\"No new journal entry files found to process.\")\n",
    "            return\n",
    "\n",
    "        # Define schema for journal entry CSVs\n",
    "        schema = StructType([\n",
    "            StructField(\"Date\", DateType(), False),\n",
    "            StructField(\"External_ID\", StringType(), False),\n",
    "            StructField(\"Account\", StringType(), True),\n",
    "            StructField(\"Debit\", DoubleType(), True),\n",
    "            StructField(\"Credit\", DoubleType(), True),\n",
    "            StructField(\"Line_Memo\", StringType(), True),\n",
    "            StructField(\"Subsidiary\", StringType(), False),\n",
    "            StructField(\"Cost_Centre\", StringType(), True),\n",
    "            StructField(\"Memo\", StringType(), True),\n",
    "            StructField(\"Reversal_Date\", DateType(), True),\n",
    "            StructField(\"Currency\", StringType(), False)\n",
    "        ])\n",
    "\n",
    "        # Define Processed Log schema\n",
    "        log_schema = StructType([\n",
    "            StructField(\"filename\", StringType(), False),\n",
    "            StructField(\"filePath\", StringType(), False),\n",
    "            StructField(\"isProcessed\", BooleanType(), False),\n",
    "            StructField(\"processedAt\", TimestampType(), False)\n",
    "        ])\n",
    "\n",
    "        # Processed Log Delta Table\n",
    "        if not DeltaTable.isDeltaTable(spark, LOG_DIR):\n",
    "            empty_log = spark.createDataFrame(data=[], schema=log_schema)\n",
    "            empty_log.coalesce(1).write.format(\"delta\").mode(\"overwrite\").save(LOG_DIR)\n",
    "        log_tbl = DeltaTable.forPath(spark, LOG_DIR)\n",
    "\n",
    "        # Set up NetSuite connection\n",
    "        base_url, oauth, headers, params = set_netsuite_connection()\n",
    "\n",
    "        # Loop through each unprocessed file\n",
    "        for row in unprocessed_df.collect():\n",
    "            filename = row.filename\n",
    "            file_path = FILE_DIR + filename\n",
    "            \n",
    "            try:\n",
    "                # Compose JSON body for POST\n",
    "                body = compose_journal_entry_post_body(file_path=file_path, schema=schema)\n",
    "                logging.info(f\"[{filename}] JSON body composed\")\n",
    "                \n",
    "                # POST to NetSuite\n",
    "                status_code = post_netsuite(endpoint=\"journalEntry\", body=body, base_url=base_url, oauth=oauth, headers=headers, params=params)\n",
    "                logging.info(f\"[{filename}] POST succeeded: {status_code}\")\n",
    "                is_processed = True\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"[{filename}] processing FAILED: {e}\")\n",
    "                is_processed = False\n",
    "\n",
    "            # Upsert processing record in Delta log\n",
    "            new_log_df = spark.createDataFrame(data=[(filename, file_path, is_processed, datetime.now(tz=ZoneInfo(\"UTC\")))], schema=log_schema)\n",
    "            (\n",
    "                log_tbl.alias(\"tar\")\n",
    "                .merge(new_log_df.alias(\"src\"), \"tar.filename = src.filename\")\n",
    "                .whenMatchedUpdateAll()\n",
    "                .whenNotMatchedInsertAll()\n",
    "                .execute()\n",
    "            )\n",
    "            logging.info(f\"[{filename}] marked the process status in log\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Catch any unexpected exception, log it, but donâ€™t skip the finally block\n",
    "        logging.exception(f\"Unexpected failure in main pipeline: {e}\")\n",
    "    \n",
    "    finally: # Always write out whatever got captured in buffered logs \n",
    "        # Extract buffered logs and persist\n",
    "        entity_logs = [\n",
    "            {\n",
    "                \"run_ts\": datetime.fromtimestamp(rec.created, tz=ZoneInfo(\"UTC\")),\n",
    "                \"level\": rec.levelname,\n",
    "                \"message\": rec.getMessage()\n",
    "            }\n",
    "            for rec in buffer_handler.buffer\n",
    "        ]\n",
    "        # save the logs\n",
    "        log_progress(log_records=entity_logs, medal=\"bronze\", subfolder_path=\"paycor\", entity=\"journalEntry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35be4479-199f-40bf-9b48-47663b75eeb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 01:47:46 INFO Received command c on object id p1\n2025-07-08 01:47:46 INFO Received command c on object id p0\n2025-07-08 01:47:47 INFO Received command c on object id p0\n2025-07-08 01:47:48 INFO Received command c on object id p0\n2025-07-08 01:47:49 INFO Python Server ready to receive messages\n2025-07-08 01:47:49 INFO Received command c on object id p0\n2025-07-08 01:47:49 INFO Received command c on object id p0\n2025-07-08 01:47:50 INFO Received command c on object id p0\n2025-07-08 01:47:51 INFO [GL_000175185_1_20250326201544953.dat] JSON body composed\n2025-07-08 01:47:51 INFO Received command c on object id p0\n2025-07-08 01:47:51 INFO Received command c on object id p0\n2025-07-08 01:47:51 INFO Received command c on object id p0\n2025-07-08 01:47:51 INFO Received command c on object id p0\n2025-07-08 01:47:51 INFO Received command c on object id p0\n2025-07-08 01:47:52 INFO Received command c on object id p0\n2025-07-08 01:47:53 INFO Received command c on object id p0\n2025-07-08 01:47:54 INFO Received command c on object id p0\n2025-07-08 01:47:55 INFO Received command c on object id p0\n2025-07-08 01:47:56 INFO Received command c on object id p0\n2025-07-08 01:47:57 INFO Received command c on object id p0\n2025-07-08 01:47:57 INFO POST journalEntry succeeded with 204\n2025-07-08 01:47:57 INFO [GL_000175185_1_20250326201544953.dat] POST succeeded: 204\n2025-07-08 01:47:58 INFO Received command c on object id p0\n2025-07-08 01:47:59 INFO Received command c on object id p0\n2025-07-08 01:48:00 INFO Received command c on object id p0\n2025-07-08 01:48:01 INFO Received command c on object id p0\n2025-07-08 01:48:02 INFO [GL_000175185_1_20250326201544953.dat] marked as processed in log\n2025-07-08 01:48:02 INFO Received command c on object id p0\n2025-07-08 01:48:03 INFO Received command c on object id p0\n2025-07-08 01:48:04 INFO Logged 26 entries for journalEntry\n2025-07-08 01:48:04 INFO Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "# === RUN PIPELINE ===\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set buffered logging: Attach the buffered ListHandler before main() so that event logs in main() can be logged\n",
    "    buffer_handler = BufferingHandler(capacity=10000)\n",
    "    buffer_handler.setLevel(logging.INFO)\n",
    "    root_logger.addHandler(buffer_handler)\n",
    "    \n",
    "    # Run\n",
    "    main()\n",
    "\n",
    "# --- End of post_netsuite_journal_entry.py ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0969f7db-3134-4aa7-8332-e69ea4c3ccad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "post_netsuite_journal_entry",
   "widgets": {}
  },
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "889fd91e-de83-4fb6-b1e0-632befda4c9e",
    "default_lakehouse_name": "qylakehouse",
    "default_lakehouse_workspace_id": "0679a537-5393-4d69-9dfc-4c327dba5458",
    "known_lakehouses": [
     {
      "id": "889fd91e-de83-4fb6-b1e0-632befda4c9e"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {
    "96ca984f-d342-44b3-8aa8-5b7f750c027e": {
     "persist_state": {
      "view": {
       "chartOptions": {
        "aggregationType": "sum",
        "binsNumber": 10,
        "categoryFieldKeys": [],
        "chartType": "bar",
        "isStacked": false,
        "seriesFieldKeys": [],
        "wordFrequency": "-1"
       },
       "tableOptions": {},
       "type": "details",
       "viewOptionsGroup": [
        {
         "tabItems": [
          {
           "key": "0",
           "name": "Table",
           "options": {},
           "type": "table"
          }
         ]
        }
       ]
      }
     },
     "sync_state": {
      "isSummary": false,
      "language": "scala",
      "table": {
       "rows": [
        {
         "0": "GL_000175185_1_20250326201544953.dat"
        }
       ],
       "schema": [
        {
         "key": "0",
         "name": "filename",
         "type": "string"
        }
       ],
       "truncated": false
      },
      "wranglerEntryContext": {
       "candidateVariableNames": [
        "unprocessed_df"
       ],
       "dataframeType": "pyspark"
      }
     },
     "type": "Synapse.DataFrame"
    }
   },
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}